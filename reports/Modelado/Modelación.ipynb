{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proyecto 3 - Modelación\n",
    "**Integrantes:**\n",
    "- Ruth de León, 22428 \n",
    "- Héctor Penedo, 22217 \n",
    "- Rodrigo Mansilla, 22611\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Respuesta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se selecciona como variable de respuesta Caudef, que corresponde al código CIE-10 de la causa de defunción. Esta variable es categórica multinivel –cada código CIE-10 representa una clase distinta– y refleja directamente el desenlace de interés, permitiendo analizar la asociación entre cada causa de muerte y factores socioeconómicos o geográficos.\n",
    "\n",
    "**Partición del conjunto de datos**\n",
    "\n",
    "Con 1 008 732 registros y 66 variables, se implementa un muestreo estratificado sobre Caudef: 70 % de los casos se destina al conjunto de entrenamiento y 30 % al de prueba. La distribución de clases presenta una cola larga: las cinco causas más frecuentes (I219: 7,13 %; J189: 5,41 %; E149: 3,67 %; R98X: 3,29 %; X599: 3,12 %) suman apenas el 22,6 % del total, mientras que el 77,4 % restante se reparte en miles de códigos con muy baja frecuencia, evidenciando un desequilibrio severo de clases.\n",
    "\n",
    "**Limpieza y reducción de dimensionalidad**\n",
    "Para abordar la alta dimensionalidad y los valores erróneos:\n",
    "\n",
    "- Se marcan como NA los valores “999”, “9999” o negativos en variables numéricas (edad, año, etc.).\n",
    "\n",
    "- Se imputan las variables numéricas con la mediana y las categóricas con la moda.\n",
    "\n",
    "- Se eliminan aquellas variables con más del 80 % de valores faltantes o con varianza prácticamente nula.\n",
    "\n",
    "**Codificación y agrupamiento**\n",
    "\n",
    "- Las variables numéricas se normalizan (centrado y escala).\n",
    "\n",
    "- Las variables categóricas se transforman mediante one-hot encoding.\n",
    "\n",
    "- Se agrupan las categorías de baja frecuencia (por ejemplo, municipios con menos del 1 % de observaciones) bajo la etiqueta “Otros” para evitar alta cardinalidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. IMPORTS Y RUTAS\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "BASE_DIR = r\"C:\\Users\\rodri\\Documents\\Data-Mining\\Proyecto-3-EDA-Clustering\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga y Eliminación de variables con observaciones nulas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rodri\\AppData\\Local\\Temp\\ipykernel_27780\\1242429795.py:2: DtypeWarning: Columns (15,29,33,34,35,52,53,54,60,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(DATA_DIR, \"master.csv\"))\n"
     ]
    }
   ],
   "source": [
    "# 1. CARGAR DATOS\n",
    "df = pd.read_csv(os.path.join(DATA_DIR, \"master.csv\"))\n",
    "X_raw = df.drop(columns=\"Caudef\")\n",
    "y_raw = df[\"Caudef\"].astype(str)   # homogéneo como string\n",
    "# 2. AGRUPAR CLASES RARAS EN y\n",
    "min_count = 2\n",
    "vc = y_raw.value_counts()\n",
    "raras = vc[vc < min_count].index\n",
    "y = y_raw.replace(raras, \"Otros\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partición estratificada 70 / 30 sobre `Caudef`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. SPLIT ESTRATIFICADO 70/30\n",
    "RANDOM_STATE = 123\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_raw, y,\n",
    "    test_size=0.30,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "# 4. IDENTIFICAR VARIABLES\n",
    "numeric_features = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "# 4.1 Forzar todas las categóricas a str\n",
    "for col in categorical_features:\n",
    "    X_train[col] = X_train[col].astype(str)\n",
    "    X_test [col] = X_test [col].astype(str)\n",
    "# 5. AGRUPAR NIVELES CATEGÓRICOS RAROS (<1% en TRAIN) EN \"Otros\"\n",
    "threshold = 0.01\n",
    "for col in categorical_features:\n",
    "    freqs = X_train[col].value_counts(normalize=True)\n",
    "    rares = freqs[freqs < threshold].index\n",
    "    X_train[col] = X_train[col].where(~X_train[col].isin(rares), other=\"Otros\")\n",
    "    X_test [col] = X_test [col].where(~X_test [col].isin(freqs.index), other=\"Otros\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de pipelines de preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. PIPELINES: IMPUTACIÓN, ESCALADO Y DUMMIES\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler' , StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot' , OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, numeric_features),\n",
    "    ('cat', cat_pipeline, categorical_features)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste del preprocesador y transformación de train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train preprocessed: (706112, 241)\n",
      "Test  preprocessed: (302620, 241)\n"
     ]
    }
   ],
   "source": [
    "# 7. AJUSTAR Y TRANSFORMAR\n",
    "X_train_prep = preprocessor.fit_transform(X_train)\n",
    "X_test_prep  = preprocessor.transform(X_test)\n",
    "\n",
    "# Reconstruir DataFrames con nombres de columnas\n",
    "ohe_cols = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)\n",
    "all_cols = numeric_features + list(ohe_cols)\n",
    "X_train_prep = pd.DataFrame(X_train_prep, columns=all_cols)\n",
    "X_test_prep  = pd.DataFrame(X_test_prep,  columns=all_cols)\n",
    "\n",
    "print(\"Train preprocessed:\", X_train_prep.shape)\n",
    "print(\"Test  preprocessed:\", X_test_prep.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar a CSV para modelado posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. GUARDAR CSVs\n",
    "os.makedirs(os.path.join(DATA_DIR, \"train\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(DATA_DIR, \"test\"),  exist_ok=True)\n",
    "\n",
    "X_train_prep.to_csv(os.path.join(DATA_DIR, \"train\", \"X_train.csv\"), index=False)\n",
    "y_train.to_frame(name=\"Caudef\").to_csv(os.path.join(DATA_DIR, \"train\", \"y_train.csv\"), index=False)\n",
    "X_test_prep .to_csv(os.path.join(DATA_DIR, \"test\" , \"X_test.csv\"),  index=False)\n",
    "y_test .to_frame(name=\"Caudef\") .to_csv(os.path.join(DATA_DIR, \"test\" , \"y_test.csv\"),  index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodología de Modelado con Random Forest (Python / Jupyter)\n",
    "\n",
    "###  Desafíos computacionales y soluciones  \n",
    "Con ~700 000 observaciones y cientos de variables, entrenar un Random Forest presenta un desafio computacional.  \n",
    "\n",
    "**Soluciones propuestas**:  \n",
    "- **Muestreo estratificado**: usar solo un 15–30 % de los datos para el _tuning_ (CV), manteniendo la proporción por `Caudef`.  \n",
    "- **Paralelización**: con `joblib` o el parámetro `n_jobs=-1` de scikit-learn, aprovechar todos los núcleos de CPU.  \n",
    "- **Reducción de árboles** en la fase de tuning (p. ej. 150–300) y simplificación de la grilla de hiperparámetros.  \n",
    "- **Menos pliegues** (5-fold CV) en lugar de 10-fold, equilibrando robustez y tiempo de cómputo.\n",
    "\n",
    "---\n",
    "\n",
    "### Trade-offs sacrificados  \n",
    "\n",
    "| Aspecto                | Pleno rendimiento    | Enfoque actual                  |\n",
    "|------------------------|----------------------|---------------------------------|\n",
    "| # árboles tuning       | 500–1 000            | ~200                            |\n",
    "| Búsqueda hiperparáms   | grillas amplías      | grillas reducidas (2–3 valores) |\n",
    "| Muestra tuning         | 100 % train set      | 15–30 % de train set            |\n",
    "| CV folds               | 10                   | 5                               |\n",
    "\n",
    "_Estos ajustes aceleran el entrenamiento, pero pueden:_  \n",
    "- Pérdida leve de precisión en la selección de hiperparámetros.  \n",
    "- Mayor varianza en la estimación CV.  \n",
    "- Menos estabilidad en la importancia de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble     import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics      import confusion_matrix, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot   as plt\n",
    "import numpy               as np\n",
    "import pandas              as pd\n",
    "# 9.2 Cargar datos procesados\n",
    "BASE = r\"C:\\Users\\rodri\\Documents\\Data-Mining\\Proyecto-3-EDA-Clustering\\data\"\n",
    "X_train = pd.read_csv(f\"{BASE}/train/X_train.csv\")\n",
    "y_train = pd.read_csv(f\"{BASE}/train/y_train.csv\")[\"Caudef\"]\n",
    "X_test  = pd.read_csv(f\"{BASE}/test/X_test.csv\")\n",
    "y_test  = pd.read_csv(f\"{BASE}/test/y_test.csv\")[\"Caudef\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Muestreo Estratificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.3 Muestreo estratificado para tuning (15 % de train)\n",
    "RND = 123\n",
    "X_tune, _, y_tune, _ = train_test_split(\n",
    "    X_train, y_train,\n",
    "    train_size    = 0.15,\n",
    "    stratify      = y_train,\n",
    "    random_state  = RND\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.4 Definir CV y número reducido de árboles\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RND)\n",
    "base_trees = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 1: Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p      = X_tune.shape[1]  \n",
    "model1 = RandomForestClassifier(  \n",
    "    n_estimators     = base_trees,  \n",
    "    max_features     = \"sqrt\",  \n",
    "    min_samples_leaf = 1,  \n",
    "    n_jobs           = -1,  \n",
    "    random_state     = RND  \n",
    ")  \n",
    "scores1 = cross_val_score(model1, X_tune, y_tune, cv=cv, scoring=\"accuracy\")  \n",
    "print(\"Modelo 1 CV Accuracy:\", np.round(scores1,3), \"→ mean =\", np.round(scores1.mean(),3))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(X_tune, y_tune)  \n",
    "y_pred1  = model1.predict(X_test)  \n",
    "y_proba1 = model1.predict_proba(X_test)  \n",
    "\n",
    "print(\"Confusion Matrix Modelo 1:\\n\", confusion_matrix(y_test, y_pred1))  \n",
    "print(classification_report(y_test, y_pred1, zero_division=0))  \n",
    "\n",
    "plt.figure(figsize=(8,6))  \n",
    "for i, cls in enumerate(model1.classes_):  \n",
    "    fpr, tpr, _ = roc_curve((y_test==cls).astype(int), y_proba1[:,i])  \n",
    "    plt.plot(fpr, tpr, label=f\"{cls} (AUC={auc(fpr,tpr):.2f})\")  \n",
    "plt.plot([0,1],[0,1],\"--\",color=\"grey\")  \n",
    "plt.title(\"ROC Modelo 1\"); plt.legend(loc=\"lower right\"); plt.show()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Modelo 2: Tuning `max_features` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid2 = {\"max_features\": [\"sqrt\",\"log2\"]}  \n",
    "gs2 = GridSearchCV(  \n",
    "    RandomForestClassifier(  \n",
    "        n_estimators     = base_trees,  \n",
    "        min_samples_leaf = 1,  \n",
    "        n_jobs           = -1,  \n",
    "        random_state     = RND  \n",
    "    ),  \n",
    "    param_grid = param_grid2,  \n",
    "    cv         = cv,  \n",
    "    scoring    = \"accuracy\",  \n",
    "    n_jobs     = -1  \n",
    ")  \n",
    "gs2.fit(X_tune, y_tune)  \n",
    "print(\"Modelo 2 best params:\", gs2.best_params_)  \n",
    "print(\"Modelo 2 CV Accuracy:\", np.round(gs2.best_score_,3))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best2    = gs2.best_estimator_  \n",
    "y_pred2  = best2.predict(X_test)  \n",
    "y_proba2 = best2.predict_proba(X_test)  \n",
    "\n",
    "print(\"Confusion Matrix Modelo 2:\\n\", confusion_matrix(y_test, y_pred2))  \n",
    "print(classification_report(y_test, y_pred2, zero_division=0))  \n",
    "\n",
    "plt.figure(figsize=(8,6))  \n",
    "for i, cls in enumerate(best2.classes_):  \n",
    "    fpr, tpr, _ = roc_curve((y_test==cls).astype(int), y_proba2[:,i])  \n",
    "    plt.plot(fpr, tpr, label=f\"{cls} (AUC={auc(fpr,tpr):.2f})\")  \n",
    "plt.plot([0,1],[0,1],\"--\",color=\"grey\")  \n",
    "plt.title(\"ROC Modelo 2\"); plt.legend(loc=\"lower right\"); plt.show()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 3: tuning max_features y min_samples_leaf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid3 = {  \n",
    "    \"max_features\"     : [\"sqrt\",\"log2\"],  \n",
    "    \"min_samples_leaf\" : [1,10]  \n",
    "}  \n",
    "gs3 = GridSearchCV(  \n",
    "    RandomForestClassifier(  \n",
    "        n_estimators = base_trees,  \n",
    "        n_jobs       = -1,  \n",
    "        random_state = RND  \n",
    "    ),  \n",
    "    param_grid = param_grid3,  \n",
    "    cv         = cv,  \n",
    "    scoring    = \"accuracy\",  \n",
    "    n_jobs     = -1  \n",
    ")  \n",
    "gs3.fit(X_tune, y_tune)  \n",
    "print(\"Modelo 3 best params:\", gs3.best_params_)  \n",
    "print(\"Modelo 3 CV Accuracy:\", np.round(gs3.best_score_,3))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best3    = gs3.best_estimator_  \n",
    "y_pred3  = best3.predict(X_test)  \n",
    "y_proba3 = best3.predict_proba(X_test)  \n",
    "\n",
    "print(\"Confusion Matrix Modelo 3:\\n\", confusion_matrix(y_test, y_pred3))  \n",
    "print(classification_report(y_test, y_pred3, zero_division=0))  \n",
    "\n",
    "plt.figure(figsize=(8,6))  \n",
    "for i, cls in enumerate(best3.classes_):  \n",
    "    fpr, tpr, _ = roc_curve((y_test==cls).astype(int), y_proba3[:,i])  \n",
    "    plt.plot(fpr, tpr, label=f\"{cls} (AUC={auc(fpr,tpr):.2f})\")  \n",
    "plt.plot([0,1],[0,1],\"--\",color=\"grey\")  \n",
    "plt.title(\"ROC Modelo 3\"); plt.legend(loc=\"lower right\"); plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Comparar distribuciones CV (boxplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.8 Comparar distribuciones CV (boxplot)\n",
    "results = pd.DataFrame({\n",
    "    \"Base\"    : cross_val_score(model1, X_tune, y_tune, cv=cv),\n",
    "    \"Tuning1\" : gs2.cv_results_[\"mean_test_score\"],\n",
    "    \"Tuning2\" : gs3.cv_results_[\"mean_test_score\"]\n",
    "})\n",
    "results.plot.box(grid=True, title=\"CV Accuracy comparado\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = gs3.best_params_  \n",
    "final_rf    = RandomForestClassifier(  \n",
    "    n_estimators     = 400,  \n",
    "    max_features     = best_params[\"max_features\"],  \n",
    "    min_samples_leaf = best_params[\"min_samples_leaf\"],  \n",
    "    n_jobs           = -1,  \n",
    "    random_state     = RND  \n",
    ")  \n",
    "final_rf.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluación en test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_f  = final_rf.predict(X_test)  \n",
    "y_proba_f = final_rf.predict_proba(X_test)  \n",
    "\n",
    "print(\"Confusion Matrix Modelo Final:\\n\", confusion_matrix(y_test, y_pred_f))  \n",
    "print(classification_report(y_test, y_pred_f, zero_division=0))  \n",
    "\n",
    "plt.figure(figsize=(8,6))  \n",
    "for i, cls in enumerate(final_rf.classes_):  \n",
    "    fpr, tpr, _ = roc_curve((y_test==cls).astype(int), y_proba_f[:,i])  \n",
    "    plt.plot(fpr, tpr, label=f\"{cls} (AUC={auc(fpr,tpr):.2f})\")  \n",
    "plt.plot([0,1],[0,1],\"--\",color=\"grey\")  \n",
    "plt.title(\"ROC Modelo Final\"); plt.legend(loc=\"lower right\"); plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importancia de las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(final_rf.feature_importances_, index=X_train.columns)  \n",
    "top20       = importances.nlargest(20)  \n",
    "top20.sort_values().plot.barh(title=\"Top 20 Importancias\"); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (venv)",
   "language": "python",
   "name": "venv312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
